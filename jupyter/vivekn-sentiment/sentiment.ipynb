{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vivekn Sentiment analysis computes Vivek Nayamanan algorithm from a set of positive and negative corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../../')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load SparkSession if not already there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ner\")\\\n",
    "    .master(\"local[1]\")\\\n",
    "    .config(\"spark.driver.memory\",\"8G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\")\\\n",
    "    .config(\"spark.jars.packages\", \"JohnSnowLabs:spark-nlp:1.6.2\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"500m\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import DocumentAssembler, Finisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a spark dataset and put it in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+\n",
      "|itemid|sentiment|                text|\n",
      "+------+---------+--------------------+\n",
      "|     1|        0|                 ...|\n",
      "|     2|        0|                 ...|\n",
      "|     3|        1|              omg...|\n",
      "|     4|        0|          .. Omga...|\n",
      "|     5|        0|         i think ...|\n",
      "|     6|        0|         or i jus...|\n",
      "|     7|        1|       Juuuuuuuuu...|\n",
      "|     8|        0|       Sunny Agai...|\n",
      "|     9|        1|      handed in m...|\n",
      "|    10|        1|      hmmmm.... i...|\n",
      "|    11|        0|      I must thin...|\n",
      "|    12|        1|      thanks to a...|\n",
      "|    13|        0|      this weeken...|\n",
      "|    14|        0|     jb isnt show...|\n",
      "|    15|        0|     ok thats it ...|\n",
      "|    16|        0|    &lt;-------- ...|\n",
      "|    17|        0|    awhhe man.......|\n",
      "|    18|        1|    Feeling stran...|\n",
      "|    19|        0|    HUGE roll of ...|\n",
      "|    20|        0|    I just cut my...|\n",
      "+------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the input data to be annotated\n",
    "data = spark. \\\n",
    "        read. \\\n",
    "        parquet(\"file:///\" + os.getcwd() + \"../../../data/sentiment.parquet\"). \\\n",
    "        limit(1000)\n",
    "data.cache()\n",
    "data.count()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We creat the document assemblerr, which will put target text column into Annotation form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the dataframe\n",
    "document_assembler = DocumentAssembler() \\\n",
    "            .setInputCol(\"text\")\n",
    "        \n",
    "### Transform input to appropriate schema\n",
    "#assembled = document_assembler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence detector will parse sub sentences in every line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sentence detector\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "#sentence_data = sentence_detector.transform(checked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer will match standard tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer\n",
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols([\"sentence\"]) \\\n",
    "            .setOutputCol(\"token\")\n",
    "#tokenized = tokenizer.transform(assembled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizer will clean out the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer() \\\n",
    "            .setInputCols([\"token\"]) \\\n",
    "            .setOutputCol(\"normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spell checker will correct normalized tokens, this trains with a dictionary of english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Spell Checker\n",
    "spell_checker = NorvigSweetingApproach() \\\n",
    "            .setInputCols([\"normal\"]) \\\n",
    "            .setOutputCol(\"spell\") \\\n",
    "            .setDictionary(\"file:///\" + os.getcwd() + \"/../../data/spell/words.txt\")\n",
    "\n",
    "#checked = spell_checker.fit(tokenized).transform(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We creat the ViveknSentimentApproach and set resources to train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_detector = ViveknSentimentApproach() \\\n",
    "    .setInputCols([\"spell\", \"sentence\"]) \\\n",
    "    .setOutputCol(\"sentiment\") \\\n",
    "    .setPruneCorpus(0) \\\n",
    "    .setPositiveSource(\"file:///\" + os.getcwd() + \"/../../data/vivekn/positive\") \\\n",
    "    .setNegativeSource(\"file:///\" + os.getcwd() + \"/../../data/vivekn/negative\") \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finisher will utilize sentiment analysis output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"sentiment\"]) \\\n",
    "    .setIncludeKeys(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy training text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|         hello world|\n",
      "|this is some more...|\n",
      "|and here another ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = spark.sparkContext.parallelize([[\"hello world\"], [\"this is some more text\"], [\"and here another sentence\"]]).toDF().toDF(\"text\")\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit and predict over data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|itemid|                text|  finished_sentiment|\n",
      "+------+--------------------+--------------------+\n",
      "|     1|                 ...|          result->na|\n",
      "|     2|                 ...|          result->na|\n",
      "|     3|              omg...|          result->na|\n",
      "|     4|          .. Omga...|result->na@result...|\n",
      "|     5|         i think ...|result->na@result...|\n",
      "|     6|         or i jus...|          result->na|\n",
      "|     7|       Juuuuuuuuu...|          result->na|\n",
      "|     8|       Sunny Agai...|          result->na|\n",
      "|     9|      handed in m...|result->na@result...|\n",
      "|    10|      hmmmm.... i...|result->na@result...|\n",
      "|    11|      I must thin...|          result->na|\n",
      "|    12|      thanks to a...|          result->na|\n",
      "|    13|      this weeken...|          result->na|\n",
      "|    14|     jb isnt show...|          result->na|\n",
      "|    15|     ok thats it ...|          result->na|\n",
      "|    16|    &lt;-------- ...|result->na@result...|\n",
      "|    17|    awhhe man.......|result->na@result...|\n",
      "|    18|    Feeling stran...|result->na@result...|\n",
      "|    19|    HUGE roll of ...|result->na@result...|\n",
      "|    20|    I just cut my...|result->na@result...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time elapsed pipeline process: 35.12329363822937\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    spell_checker,\n",
    "    sentiment_detector,\n",
    "    finisher\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "sentiment_data = pipeline.fit(training).transform(data)\n",
    "sentiment_data.show()\n",
    "end = time.time()\n",
    "print(\"Time elapsed pipeline process: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a sample back into the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(itemid=1, text='                     is so sad for my APL friend.............', finished_sentiment='result->na')\n",
      "Row(itemid=2, text='                   I missed the New Moon trailer...', finished_sentiment='result->na')\n",
      "Row(itemid=3, text='              omg its already 7:30 :O', finished_sentiment='result->na')\n",
      "Row(itemid=4, text=\"          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\", finished_sentiment='result->na@result->na@result->na@result->negative')\n",
      "Row(itemid=5, text='         i think mi bf is cheating on me!!!       T_T', finished_sentiment='result->na@result->na')\n"
     ]
    }
   ],
   "source": [
    "for r in sentiment_data.take(5):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save it to disk and re read it. Either after or before fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed in write pipelines: 34.681615352630615\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pipeline.write().overwrite().save(\"./ps\")\n",
    "pipeline.fit(data).write().overwrite().save(\"./ms\")\n",
    "end = time.time()\n",
    "print(\"Time elapsed in write pipelines: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed in read pipelines: 23.841302633285522\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "p = Pipeline.read().load(\"./ps\")\n",
    "pm = PipelineModel.read().load(\"./ms\")\n",
    "end = time.time()\n",
    "print(\"Time elapsed in read pipelines: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|itemid|                text|  finished_sentiment|\n",
      "+------+--------------------+--------------------+\n",
      "|     1|                 ...|          result->na|\n",
      "|     2|                 ...|          result->na|\n",
      "|     3|              omg...|          result->na|\n",
      "|     5|         i think ...|result->na@result...|\n",
      "|     6|         or i jus...|          result->na|\n",
      "|     7|       Juuuuuuuuu...|          result->na|\n",
      "|     8|       Sunny Agai...|          result->na|\n",
      "|     9|      handed in m...|result->na@result...|\n",
      "|    10|      hmmmm.... i...|result->na@result...|\n",
      "|    11|      I must thin...|          result->na|\n",
      "|    12|      thanks to a...|          result->na|\n",
      "|    13|      this weeken...|          result->na|\n",
      "|    14|     jb isnt show...|          result->na|\n",
      "|    15|     ok thats it ...|          result->na|\n",
      "|    16|    &lt;-------- ...|result->na@result...|\n",
      "|    17|    awhhe man.......|result->na@result...|\n",
      "|    18|    Feeling stran...|result->na@result...|\n",
      "|    19|    HUGE roll of ...|result->na@result...|\n",
      "|    20|    I just cut my...|result->na@result...|\n",
      "|    21|    Very sad abou...|          result->na|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "1000\n",
      "Time elapsed in using loaded pipelines: 4.993432283401489\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pm.transform(data).where(\"finished_sentiment not like '%negative%'\").show()\n",
    "print(pm.transform(data).count())\n",
    "end = time.time()\n",
    "print(\"Time elapsed in using loaded pipelines: \" + str(end - start))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
